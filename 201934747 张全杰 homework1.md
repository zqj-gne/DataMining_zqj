# Data mining  #
## Homework1: Clustering with sklearn ##
- 学院：计算机科学与技术学院
- 班级：2019级专硕班
- 姓名：张全杰
- 学号：201934747
- 任课教师：尹建华

## 实验要求 ##
运用数字数据集和新闻数据集，测试sklearn中以下聚类算法在以上两个数据集上的聚类效果。
![Image text](https://github.com/zqj-gne/DataMining_zqj/blob/master/img/methods.jpg)


## 实验环境 ##
处理器： Intel(R) Core(TM) i5-4200H CPU @ 2.80Ghz 2.79Ghz

系统： Win10，64 位

编程语言： Python 3.7

IDE: PyCharm
## 实验代码 ##
####数据集1
    import warnings
    warnings.filterwarnings("ignore")
    
    np.random.seed(42)
    
    digits = load_digits()
    data = scale(digits.data)
    
    n_samples, n_features = data.shape
    n_digits = len(np.unique(digits.target))
    labels = digits.target
    
    sample_size = 300
    
    print("n_digits: %d, \t n_samples %d, \t n_features %d"
      % (n_digits, n_samples, n_features))
    
    print(82 * '_')
    print('init\t\t\t\t\t\ttime\tNMI\t\thomo\tcompl')
    
    
    def bench(estimator, name, data):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t\t\t\t\t%.2fs\t%.3f\t%.3f\t%.3f'
      % (name, (time() - t0),
     metrics.normalized_mutual_info_score(labels, estimator.labels_),
     metrics.homogeneity_score(labels, estimator.labels_),
     metrics.completeness_score(labels, estimator.labels_)))
    
    
    bench(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
      name="k-means++", data=data)
    
    bench(KMeans(init='random', n_clusters=n_digits, n_init=10),
      name="random", data=data)
    
    # in this case the seeding of the centers is deterministic, hence we run the
    # kmeans algorithm only once with n_init=1
    pca = PCA(n_components=n_digits).fit(data)
    bench(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
      name="PCA-based",
      data=data)
    
    #Affinity Propagation
    bench(AffinityPropagation(preference=10),
      name="Affinity Propagation",data=data)
    
    #DBSCAN
    bench(DBSCAN(eps = 1, min_samples = 1),
      name="DBSCAN",data=data)
    
    print(82 * '_')
####数据集2
    logging.basicConfig(level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s')
    
    # parse commandline arguments
    op = OptionParser()
    op.add_option("--lsa",
      dest="n_components", type="int",
      help="Preprocess documents with latent semantic analysis.")
    op.add_option("--no-minibatch",
      action="store_false", dest="minibatch", default=True,
      help="Use ordinary k-means algorithm (in batch mode).")
    op.add_option("--no-idf",
      action="store_false", dest="use_idf", default=True,
      help="Disable Inverse Document Frequency feature weighting.")
    op.add_option("--use-hashing",
      action="store_true", default=False,
      help="Use a hashing feature vectorizer")
    op.add_option("--n-features", type=int, default=10000,
      help="Maximum number of features (dimensions)"
       " to extract from text.")
    op.add_option("--verbose",
      action="store_true", dest="verbose", default=False,
      help="Print progress reports inside k-means algorithm.")
    
    print(__doc__)
    op.print_help()
    
    
    def is_interactive():
    return not hasattr(sys.modules['__main__'], '__file__')
    
    
    # work-around for Jupyter notebook and IPython console
    argv = [] if is_interactive() else sys.argv[1:]
    (opts, args) = op.parse_args(argv)
    if len(args) > 0:
    op.error("this script takes no arguments.")
    sys.exit(1)
    
    
    # #############################################################################
    # Load some categories from the training set
    categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
    ]
    # Uncomment the following to do the analysis on all the categories
    # categories = None
    
    print("Loading 20 newsgroups dataset for categories:")
    print(categories)
    
    dataset = fetch_20newsgroups(subset='all', categories=categories,
     shuffle=True, random_state=42)
    
    print("%d documents" % len(dataset.data))
    print("%d categories" % len(dataset.target_names))
    print()
    
    labels = dataset.target
    true_k = np.unique(labels).shape[0]
    
    print("Extracting features from the training dataset "
      "using a sparse vectorizer")
    t0 = time()
    if opts.use_hashing:
    if opts.use_idf:
    # Perform an IDF normalization on the output of HashingVectorizer
    hasher = HashingVectorizer(n_features=opts.n_features,
       stop_words='english', alternate_sign=False,
       norm=None, binary=False)
    vectorizer = make_pipeline(hasher, TfidfTransformer())
    else:
    vectorizer = HashingVectorizer(n_features=opts.n_features,
       stop_words='english',
       alternate_sign=False, norm='l2',
       binary=False)
    else:
    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,
     min_df=2, stop_words='english',
     use_idf=opts.use_idf)
    X = vectorizer.fit_transform(dataset.data)
    
    print("done in %fs" % (time() - t0))
    print("n_samples: %d, n_features: %d" % X.shape)
    print()
    
    if opts.n_components:
    print("Performing dimensionality reduction using LSA")
    t0 = time()
    # Vectorizer results are normalized, which makes KMeans behave as
    # spherical k-means for better results. Since LSA/SVD results are
    # not normalized, we have to redo the normalization.
    svd = TruncatedSVD(opts.n_components)
    normalizer = Normalizer(copy=False)
    lsa = make_pipeline(svd, normalizer)
    
    X = lsa.fit_transform(X)
    
    print("done in %fs" % (time() - t0))
    
    explained_variance = svd.explained_variance_ratio_.sum()
    print("Explained variance of the SVD step: {}%".format(
    int(explained_variance * 100)))
    
    print()
    
    
    # #############################################################################
    # Do the actual clustering
    print('init\t\t\t\t\t\ttime\tNMI\t\thomo\tcompl')
    def bench(estimator, name, data):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t\t\t\t\t%.2fs\t%.3f\t%.3f\t%.3f'
      % (name, (time() - t0),
     metrics.normalized_mutual_info_score(labels, estimator.labels_),
     metrics.homogeneity_score(labels, estimator.labels_),
     metrics.completeness_score(labels, estimator.labels_)))
    
    if opts.minibatch:
    bench(MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=opts.verbose),
      name="MiniBatchKMeans", data=X)
    else:
    bench(KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=opts.verbose),
      name="KMeans", data=X)
    
    #Affinity Propagation
    bench(AffinityPropagation(),
      name="Affinity Propagation", data=X)
    
    #MeanShift
    bench(MeanShift(),
      name="MeanShift", data=X)
    
    #Spectral Clustering
    bench(SpectralClustering(),
      name="Spectral Clustering", data=X)
    
    #Agglomerative Clustering
    bench(AgglomerativeClustering(),
      name="Agglomerative Clustering", data=X)
    
    #DBSCAN
    bench(DBSCAN(eps = 1, min_samples = 1),
      name="DBSCAN", data=X)
## 实验结果 ##
####数据集1
- init&emsp;time&emsp;NMI&emsp;homo&emsp;compl
- k-means++					0.97s	0.626	0.602	0.650
- random   					0.72s	0.689	0.669	0.710
- PCA-based					0.12s	0.681	0.667	0.695
- Affinity Propagation		5.33s	0.554  	1.000	0.307
- MeanShift					30.82s	0.063	0.014	0.281
- Agglomerative Clustering	1.25s	0.065	0.017	0.249
- DBSCAN   					1.09s	0.554	1.000	0.307
####数据集2
Loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
3387 documents

n_samples: 3387, n_features: 10000

- init&emsp;time&emsp;NMI&emsp;homo&emsp;compl
- MiniBatchKMeans	0.32s	0.564	0.543	0.585
- KMeans 	27.88s	0.474	0.404	0.555
- Affinity Propagation		33.94s	0.411	0.885	0.191
- DBSCAN   		1.12s	0.422	0.945	0.188