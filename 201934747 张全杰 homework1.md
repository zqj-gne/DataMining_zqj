# Data mining  #
## Homework1: Clustering with sklearn ##
- 学院：计算机科学与技术学院
- 班级：2019级专硕班
- 姓名：张全杰
- 学号：201934747
- 任课教师：尹建华

## 实验要求 ##
运用数字数据集和新闻数据集，测试sklearn中以下聚类算法在以上两个数据集上的聚类效果。
![Image text](https://github.com/zqj-gne/DataMining_zqj/blob/master/img/methods.jpg)


## 实验环境 ##
处理器： Intel(R) Core(TM) i5-4200H CPU @ 2.80Ghz 2.79Ghz

系统： Win10，64 位

编程语言： Python 3.7

IDE: PyCharm
## 实验内容 ##
本次实验使用两个数据集。数据集1是手写数字的数据集，数据集中包含1797个样本，text格式，整体分为10个类别，每个类别约180个样本，数据维度为64.数据集2是新闻数据集，包含18846个样本，整体分为20类，数据维度为1。

运用sklearn的多种聚类方法对这两个样本分析，包括K-Means、Affinity propagation、Mean-shift、Spectral clustering、Agglomerative clustering、DBSCAN、Gaussian mixtures这些方法。

对运行结果的分析采用NMI、Homogeneity、Completeness三种方法。

基于归一化互信息（NMI）的得分：

假设两个标签分配（在相同的N个对象中进行）U和V。它们的熵(entropy)是一个分区集合(partition set)的不确定性量，定义如下:

H(U) = sum{i=1}^{|U|}P(i)*log(P(i))

其中P(i) = |U_i|/N是从U中随机选取的对象,选取对象落入到类U_i的概率。同样对于V:

H(V) = sum_{j=1}^{|V|}P'(j)*log(P'(j))

使用P'(j) = |V_j|/N. U和V之间的mutual information(MI)由下式计算:

MI(U, V) = sum{i=1}^{|U|}sum_{j=1}^{|V|}P(i, j)*log(P(i,j)/(P(i)P'(j)))

其中 P(i, j)=|U_i  V_j|/N 是随机选择的对象同时落入两个类的概率U_i和V_j。

也可以用设定的基数表达式表示:

MI(U, V) = sum_{i=1}^{|U|} sum_{j=1}^{|V|} (|U_i  V_j|/N)*log(N|U_i  V_j|)/(|U_i||V_j|))

归一化(normalized) mutual information 被定义为

NMI(U, V) = MI(U, V)/((H(U)H(V))^0.5)

mutual information 的值以及归一化变量(normalized variant)的值都不会为随机标签度量而调整，但是会随着不同的簇标签数量的增加，不管标签分配之间的 “mutual information” 的实际数量如何，都会趋向于增加。

同质性(homogeneity): 每个簇只包含一个类的成员。

完整性(completeness): 给定类的所有成员都分配给同一个簇。

同质性和完整性的得分由下面公式给出:

h = 1 - H(C|K)/H(C)

c = 1 - H(K|C)/H(K)

其中 H(C|K)是给定簇分配的类的条件熵，由下式给出:

H(C|K) = sum_{c=1}^{|C|} sum_{k=1}^{|K|}（n_{c,k}/n）*log（n_{c,k}}/{n_k})

并且 H(C)是已聚合的类的熵(entropy of the classes)，并且由下式给出:

H(C) = sum{c=1}^{|C|} （n_c/n)*log（n_c/n)

n个样本总数，n_c和n_k分别属于c类和簇k的样本数，最后 n_{c,k}分配给簇k的类c的样本数。

给定类分配的簇的条件熵（conditional entropy of clusters given class）H(K|C)和簇的熵 H(K)以对称的形式定义。
## 实验代码 ##
<<<<<<< HEAD
### 数据集1 ###
=======

####数据集1####
>>>>>>> 55133d2348403b2ec415a93d49eaec7179ac7ff5

    import warnings
    warnings.filterwarnings("ignore")
    
    np.random.seed(42)
    
    digits = load_digits()
    data = scale(digits.data)
    
    n_samples, n_features = data.shape
    n_digits = len(np.unique(digits.target))
    labels = digits.target
    
    sample_size = 300
    
    print("n_digits: %d, \t n_samples %d, \t n_features %d"
      % (n_digits, n_samples, n_features))
    
    print(82 * '_')
    print('init\t\t\t\t\t\ttime\tNMI\t\thomo\tcompl')
    
    
    def bench(estimator, name, data):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t\t\t\t\t%.2fs\t%.3f\t%.3f\t%.3f'
      % (name, (time() - t0),
     metrics.normalized_mutual_info_score(labels, estimator.labels_),
     metrics.homogeneity_score(labels, estimator.labels_),
     metrics.completeness_score(labels, estimator.labels_)))
    
    
    bench(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
      name="k-means++", data=data)
    
    bench(KMeans(init='random', n_clusters=n_digits, n_init=10),
      name="random", data=data)
    
    # in this case the seeding of the centers is deterministic, hence we run the
    # kmeans algorithm only once with n_init=1
    pca = PCA(n_components=n_digits).fit(data)
    bench(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
      name="PCA-based",
      data=data)
    
    #Affinity Propagation
    bench(AffinityPropagation(preference=10),
      name="Affinity Propagation",data=data)
    
    #DBSCAN
    bench(DBSCAN(eps = 1, min_samples = 1),
      name="DBSCAN",data=data)
    
    print(82 * '_')
<<<<<<< HEAD
### 数据集2 ###
=======
####数据集2####
>>>>>>> 55133d2348403b2ec415a93d49eaec7179ac7ff5

    logging.basicConfig(level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s')
    
    # parse commandline arguments
    op = OptionParser()
    op.add_option("--lsa",
      dest="n_components", type="int",
      help="Preprocess documents with latent semantic analysis.")
    op.add_option("--no-minibatch",
      action="store_false", dest="minibatch", default=True,
      help="Use ordinary k-means algorithm (in batch mode).")
    op.add_option("--no-idf",
      action="store_false", dest="use_idf", default=True,
      help="Disable Inverse Document Frequency feature weighting.")
    op.add_option("--use-hashing",
      action="store_true", default=False,
      help="Use a hashing feature vectorizer")
    op.add_option("--n-features", type=int, default=10000,
      help="Maximum number of features (dimensions)"
       " to extract from text.")
    op.add_option("--verbose",
      action="store_true", dest="verbose", default=False,
      help="Print progress reports inside k-means algorithm.")
    
    print(__doc__)
    op.print_help()
    
    
    def is_interactive():
    return not hasattr(sys.modules['__main__'], '__file__')
    
    
    # work-around for Jupyter notebook and IPython console
    argv = [] if is_interactive() else sys.argv[1:]
    (opts, args) = op.parse_args(argv)
    if len(args) > 0:
    op.error("this script takes no arguments.")
    sys.exit(1)
    
    
    # #############################################################################
    # Load some categories from the training set
    categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
    ]
    # Uncomment the following to do the analysis on all the categories
    # categories = None
    
    print("Loading 20 newsgroups dataset for categories:")
    print(categories)
    
    dataset = fetch_20newsgroups(subset='all', categories=categories,
     shuffle=True, random_state=42)
    
    print("%d documents" % len(dataset.data))
    print("%d categories" % len(dataset.target_names))
    print()
    
    labels = dataset.target
    true_k = np.unique(labels).shape[0]
    
    print("Extracting features from the training dataset "
      "using a sparse vectorizer")
    t0 = time()
    if opts.use_hashing:
    if opts.use_idf:
    # Perform an IDF normalization on the output of HashingVectorizer
    hasher = HashingVectorizer(n_features=opts.n_features,
       stop_words='english', alternate_sign=False,
       norm=None, binary=False)
    vectorizer = make_pipeline(hasher, TfidfTransformer())
    else:
    vectorizer = HashingVectorizer(n_features=opts.n_features,
       stop_words='english',
       alternate_sign=False, norm='l2',
       binary=False)
    else:
    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,
     min_df=2, stop_words='english',
     use_idf=opts.use_idf)
    X = vectorizer.fit_transform(dataset.data)
    
    print("done in %fs" % (time() - t0))
    print("n_samples: %d, n_features: %d" % X.shape)
    print()
    
    if opts.n_components:
    print("Performing dimensionality reduction using LSA")
    t0 = time()
    # Vectorizer results are normalized, which makes KMeans behave as
    # spherical k-means for better results. Since LSA/SVD results are
    # not normalized, we have to redo the normalization.
    svd = TruncatedSVD(opts.n_components)
    normalizer = Normalizer(copy=False)
    lsa = make_pipeline(svd, normalizer)
    
    X = lsa.fit_transform(X)
    
    print("done in %fs" % (time() - t0))
    
    explained_variance = svd.explained_variance_ratio_.sum()
    print("Explained variance of the SVD step: {}%".format(
    int(explained_variance * 100)))
    
    print()
    
    
    # #############################################################################
    # Do the actual clustering
    print('init\t\t\t\t\t\ttime\tNMI\t\thomo\tcompl')
    def bench(estimator, name, data):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t\t\t\t\t%.2fs\t%.3f\t%.3f\t%.3f'
      % (name, (time() - t0),
     metrics.normalized_mutual_info_score(labels, estimator.labels_),
     metrics.homogeneity_score(labels, estimator.labels_),
     metrics.completeness_score(labels, estimator.labels_)))
    
    if opts.minibatch:
    bench(MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=opts.verbose),
      name="MiniBatchKMeans", data=X)
    else:
    bench(KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=opts.verbose),
      name="KMeans", data=X)
    
    #Affinity Propagation
    bench(AffinityPropagation(),
      name="Affinity Propagation", data=X)
    
    #MeanShift
    bench(MeanShift(),
      name="MeanShift", data=X)
    
    #Spectral Clustering
    bench(SpectralClustering(),
      name="Spectral Clustering", data=X)
    
    #Agglomerative Clustering
    bench(AgglomerativeClustering(),
      name="Agglomerative Clustering", data=X)
    
    #DBSCAN
    bench(DBSCAN(eps = 1, min_samples = 1),
      name="DBSCAN", data=X)
## 实验结果 ##
<<<<<<< HEAD
### 数据集1 ###
=======

####数据集1####

>>>>>>> 55133d2348403b2ec415a93d49eaec7179ac7ff5
- init&emsp;time&emsp;NMI&emsp;homo&emsp;compl
- k-means++					0.97s	0.626	0.602	0.650
- random   					0.72s	0.689	0.669	0.710
- PCA-based					0.12s	0.681	0.667	0.695
- Affinity Propagation		5.33s	0.554  	1.000	0.307
- MeanShift					30.82s	0.063	0.014	0.281
- Agglomerative Clustering	1.25s	0.065	0.017	0.249
- DBSCAN   					1.09s	0.554	1.000	0.307
<<<<<<< HEAD
### 数据集2 ###
=======

####数据集2####

>>>>>>> 55133d2348403b2ec415a93d49eaec7179ac7ff5
Loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
3387 documents

n_samples: 3387, n_features: 10000

- init&emsp;time&emsp;NMI&emsp;homo&emsp;compl
- MiniBatchKMeans	0.32s	0.564	0.543	0.585
- KMeans 	27.88s	0.474	0.404	0.555
- Affinity Propagation		33.94s	0.411	0.885	0.191
- DBSCAN   		1.12s	0.422	0.945	0.188
