# Data mining  #
## Homework1: Clustering with sklearn ##
- 学院：计算机科学与技术学院
- 班级：2019级专硕班
- 姓名：张全杰
- 学号：201934747
- 任课教师：尹建华

## 实验要求 ##
运用数字数据集和新闻数据集，测试sklearn中以下聚类算法在以上两个数据集上的聚类效果。
![Image text](https://i.imgur.com/cf1wf7R.jpg)


## 实验环境 ##
处理器： Intel(R) Core(TM) i5-4200H CPU @ 2.80Ghz 2.79Ghz

系统： Win10，64 位

编程语言： Python 3.7

IDE: PyCharm
## 实验代码 ##
    print(__doc__)
    
    from time import time
    import numpy as np
    import matplotlib.pyplot as plt
    
    from sklearn import metrics
    from sklearn.cluster import KMeans
    from sklearn.cluster import AffinityPropagation
    from sklearn.cluster import MeanShift
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.cluster import SpectralClustering
    from sklearn.cluster import DBSCAN
    from sklearn.mixture import GaussianMixture
    from sklearn.datasets import load_digits
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import scale
    
    import warnings
    warnings.filterwarnings("ignore")
    
    np.random.seed(42)
    
    digits = load_digits()
    data = scale(digits.data)
    
    n_samples, n_features = data.shape
    n_digits = len(np.unique(digits.target))
    labels = digits.target
    
    sample_size = 300
    
    print("n_digits: %d, \t n_samples %d, \t n_features %d"
      % (n_digits, n_samples, n_features))
    
    print(82 * '_')
    print('init\t\t\t\t\t\ttime\tNMI\t\thomo\tcompl')
    
    
    def bench(estimator, name, data):
    t0 = time()
    estimator.fit(data)
    print('%-9s\t\t\t\t\t%.2fs\t%.3f\t%.3f\t%.3f'
      % (name, (time() - t0),
     metrics.normalized_mutual_info_score(labels, estimator.labels_),
     metrics.homogeneity_score(labels, estimator.labels_),
     metrics.completeness_score(labels, estimator.labels_)))
    
    
    bench(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
      name="k-means++", data=data)
    
    bench(KMeans(init='random', n_clusters=n_digits, n_init=10),
      name="random", data=data)
    
    # in this case the seeding of the centers is deterministic, hence we run the
    # kmeans algorithm only once with n_init=1
    pca = PCA(n_components=n_digits).fit(data)
    bench(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
      name="PCA-based",
      data=data)
    
    #Affinity Propagation
    bench(AffinityPropagation(preference=10),
      name="Affinity Propagation",data=data)
    
    #MeanShift
    bench(MeanShift(),
      name="MeanShift",data=data)
    
    # #Spectral Clustering
    # bench(SpectralClustering(),
    #   name="Spectral Clustering",data=data)
    
    #Agglomerative Clustering
    bench(AgglomerativeClustering(n_clusters=n_digits, affinity='euclidean', linkage='complete'),
      name="Agglomerative Clustering",data=data)
    
    #DBSCAN
    bench(DBSCAN(eps = 0.1, min_samples = 10),
      name="DBSCAN",data=data)
    
    print(82 * '_')
## 实验结果 ##
- init						time	NMI		homo	compl
- k-means++					0.97s	0.626	0.602	0.650
- random   					0.72s	0.689	0.669	0.710
- PCA-based					0.12s	0.681	0.667	0.695
- Affinity Propagation		5.33s	0.554	1.000	0.307
- MeanShift					30.82s	0.063	0.014	0.281
- Agglomerative Clustering	1.25s	0.065	0.017	0.249
- DBSCAN   					1.38s	0.375	0.000	1.000






